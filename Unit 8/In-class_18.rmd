---
title: "Class 18: Leverage and Influence"
author: "Hughes/Fisher"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(tidyverse)
library(ggfortify)
library(gridExtra)
library(knitr)
```

## Data

We have two files of interest. The file `firearmsMurderOwnship.csv` contains data from a 2007 United Nations study where they collected data on the homicide rate (per 100,000 people) and average firearm ownership per household of a particular country. The file `iahd-index.csv` contains the Inequality-adjusted Human Development Index (HDI) values for countries around the world. HDI is a measure of the *development* of a country measuring values such as life expentences, education levels, standards of living (per capita income, cost of living, etc...); higher values indicates a more *developed* country. This particular measure was collected circa 2010.

Today we will explore these data as they provide an interesting study of leverage, influence and potential alternatives to regular linear regression.

Specifically, we will consider the homicide rate as the response variable ($Y$) with gun ownership as the predictor variable ($X$) for countries with an IHDI value greater than 0.75. This corresponds to the 32 most *developed* countires in the world at the time the data was collected. 

**Note**: No politics today, either interational or nationally. Just an interesting dataset. 

### Get the Gun Data

First we need to get the two datasets and merge them by country. 

```{r}
guns <- read.csv("firearmsMurderOwnership.csv")
names(guns)[c(1,6,8)] <- c("Country", "Homicides", "Ownership")
guns <- guns %>% select(Country, Homicides, Ownership)

iahdi <- read.csv("iahd-index.csv", skip=5, na.strings="..")
names(iahdi)[1:5] <- c("HDI Rank", "Country", "HDI", "NULL", "IHDI")
iahdi <- iahdi %>% select(Country, IHDI)

full.guns <- inner_join(guns, iahdi, by="Country")
```

A note about `inner_join`: If you have taken a course that involves SQL then you already know! If you have not, `inner_join` essentially says to merge the dataset `guns` with `iahdi` using `Country` as the common key but only do so when the corresponding `Country` is in both datasets (essentially the intersection). So the IHDI value for Uzbekistan will be linked with the homicide rate and gun ownership rates of Uzbekistan. Only countries with values in both datasets will be linked (hence the *inner* part of `inner_join`).

Now filter to countries with an IHDI of greater than 0.75. Then plot the data we will study.

```{r}
developed.country <- full.guns %>% 
  filter(IHDI>0.75)
head(developed.country)

ggplot(developed.country, aes(x=Ownership, y=Homicides)) + 
  geom_point() +
  geom_text(aes(x=Ownership, y=Homicides, label=Country), nudge_x=0, nudge_y=0.1, size=3) + 
  xlim(c(-5,90)) + xlab("Average Firearms per 100 Households") +
  ylab("Homicides by Firearm per 100,000 People") +
  theme_classic()
```

We see the United States stands on it is own! In terms of both the $x$-axis and $y$-axis. It is likely an outlier in both cases. Let's check with some simple Box-Whiskers plots.

```{r}
p1 <- ggplot(developed.country) + 
  geom_boxplot(aes(x="", y=Ownership) ) +
  labs(title="Gun Ownership") + xlab("") + 
  theme_classic()
p2 <- ggplot(developed.country) + 
  geom_boxplot(aes(x="", y=Homicides) ) +
  labs(title="Homicide Rate") + xlab("") + 
  theme_classic()
grid.arrange(p1, p2, nrow=1)
```

We see the United States values are outliers in both cases. Let's keep that in mind.

### Regression

We fit a regression looking to predict Homicide Rate based on gun ownership.

```{r models}
full.fit <- lm(Homicides ~ Ownership, data=developed.country)
autoplot(full.fit)
```

Looks like there are some major concerns regarding the residuals. Potential issues with linearity (line in the Residuals vs Fitted plot), constant variance (large increases in Scale-Location plot) and normality (observation 32 potentially influence normality). Looking at the Residuals vs Leverage plot, we also see observation 32 appears to have large Leverage and is an outlier in terms of standardized residuals ($z$-score greater than 4). And not surprising based on the EDA above, observation 32 is...

```{r}
developed.country %>% 
  filter(row_number()==32) %>%
  select(Country)
```

So the United States looked like an outlier in our initial graphical assessment, and now it appears to be messing up our regression assumptions. For now, let's take a look at the regression output and the fitted line.

```{r}
summary(full.fit)
```

It appears that Gun ownership is a significant predictor ($F$-test of 34.88 on 1 and 30 degrees of freedom, $p$-value=$10^{-6}$) for the murder rate of a country, explaining a little over 50\% of the variability in homicide rates.

Below is a clever way to plot the fitted line. A couple of notes about this code:

1. In the `geom_line` statement, specify the data as being the `lm` object, here an object called `full.fit`. For the $y$-axis, tell it to use the `.fitted` component of the `lm` object, which will use the fitted values from the model.
2. We use the `geom_text` function to put text in the location of points, where the label of the text is the Country. We *nudge* the label away from the point so it does not overlap. We did this above as well without explanation.

```{r}
ggplot(developed.country) + 
  geom_line(data=full.fit, aes(x=Ownership, y=.fitted), col="blue", size=1.15) +
  geom_point(aes(x=Ownership, y=Homicides)) +
  geom_text(aes(x=Ownership, y=Homicides, label=Country), nudge_x=0, nudge_y=0.1, size=3) + 
  xlim(c(-5,90)) + xlab("Average Firearms per 100 people") +
  ylab("Homicides by firearm per 100,000 people") +
  theme_classic()
```

Surely, it looks as if Gun Ownership predicts the homicide rate. But we also see there is a clear outlier and based on our residuals analysis, that outlier may be causing violations in our regression. Let's dig into this further...

### A "deeper dive" into the problem

Continuing with the analysis above, let's do the following:

1. Create a dataset where we remove only the United States, and then fit a linear regression model predicting homicides as a function of gun ownership for this data. 
2. Create a dataset where you remove only Sweden from the original data (i.e. keep the United States in the dataset!). Fit a linear regression model predicting homicides as a function of gun ownership. Compare/constrast this fitted model with the "no US" model in part 1.
3. Create a dataset where you remove only Japan (keeping the United States and Sweden). Fit a linear regression model predicting homicides as a function of gun ownership. Compare/constrast this fitted model with the "no US" and "no Sweden" models.
4. Make a plot with the fitted lines from parts 1, 2 and 3 above along with the original fit we provided above. That is, a plot of the 32 observations with 4 lines - 1 being the original fit (blue line above), and the other three being those fit in parts 1, 2, and 3 here. Compare/constrast the visual lines.  We will use different colors so we can distinghuish the lines.

```{r}
# Create the data sets for questions 1-3
no.us <- developed.country %>% filter(Country != "United States")
no.sweden <- developed.country %>% filter(Country != "Sweden")
no.japan <- developed.country %>% filter(Country != "Japan")
  
# Fit the 3 models
no.us.fit <- lm(Homicides ~ Ownership, data=no.us)
no.sweden.fit <- lm(Homicides ~ Ownership, data=no.sweden)
no.japan.fit <- lm(Homicides ~ Ownership, data=no.japan) 

# Check the model output
summary(no.us.fit)
summary(no.sweden.fit)
summary(no.japan.fit)
```

*What do we observe?* **The models without Japan and Sweden are similar to the full model. However, the model without the United States has a much shallower slope. (In fact, the coefficient for the slope is no longer significant in the "no US" model.)**

Here's the code to make the plot:

```{r}
ggplot(developed.country) + 
  geom_line(data=full.fit, aes(x=Ownership, y=.fitted), col="blue", size=1) +
  geom_line(data=no.us.fit, aes(x=Ownership, y=.fitted), col="red", size=1) +
  geom_line(data=no.sweden.fit, aes(x=Ownership, y=.fitted), col="orange", size=1) +
  geom_line(data=no.japan.fit, aes(x=Ownership, y=.fitted), col="green", size=1) +
  geom_point(aes(x=Ownership, y=Homicides)) +
  geom_text(aes(x=Ownership, y=Homicides, label=Country), nudge_x=0, nudge_y=0.1, size=3) + 
  xlim(c(-5,90)) + xlab("Average Firearms per 100 people") +
  ylab("Homicides by firearm per 100,000 people") +
  theme_classic()
```

**The lines for the full model, the "no-Sweden" model, and the "no-Japan" model are all almost identical. We can see graphically that the line for the "no-US" model is much closer to horizontal.**

----

### Leverage and Influence

This data example contains a classic case of an overly **influential point**. We can assess overly influential points in a number of different ways. 

We *could* spend weeks on the topic of detecting overly influential and high leverage points. We do not want to! 

A few key measures we will consider are Cook's Distance (also known as Cook's D) and the Hat values. Cook's D provides an overall measure of how influential a point is to your regression (that is, how it influences the fitted model). The Hat values provide a measure of how extreme the point is in terms of other predictor values (essentially this is the measure for Leverage). 

Other measures include the DF-Fit and DF-$\beta$ values. You essentially explored these in the questions above (when you compared/contrasted the three fitted lines). Larger DF-Fit or DF-$\beta$ magnitudes indicate a particular variable may be overly influential. You should have noted based on your analysis that the United States appears to greatly influence the regression fit.

We can calculate all these measures using the function `influence.measures()`.

```{r}
influence.measures(full.fit)
```

Note that this function is verbose. Be careful using this function (if you had 50,000 observations, imagine the output!!). 

There is no set rule on what a *bad* Cook's D value or Hat value is. It's all relative. You'll note in the above that observation 32 (the United States) has a Cook's D value 10.4 when every other point has a value of 0.09 or smaller. Clearly point 32 is highly influential. You'll also note its hat value is 0.5095 when the next largest is about 0.09. Again, it is standing out. R provides a `*` for the United States under the `inf` column indicating it is likely overly influential.

Likewise, when looking the `dfb.` and `dffit` values, you see that the values for the United States are extreme relative to the other variables. We can also explore these graphically. 

The function `autoplot` for an `lm` object normally displays four plots (we did it earlier). It actually generates 6 plots, but only displays 4 by default. I can tell it to display all 6 or some combination with the `which` option:

```{r}
autoplot(full.fit, which=1:6)
```

We see there are two new plots here, one labeled `Cook's distance` and another `Cook's dist vs Leverage`. Let's take a closer look at these two plots, plots number 4 and 6:

```{r}
autoplot(full.fit, which=c(4,6))
```

**Certainly plots of these kinds of diagnositics are far more informative than a table.** The Cook's distance plot draws vertical lines for each observation with its associated Cook's D value. Similar to the table above, we are looking for values that look different than everything else. Here, observation 32 really jumps out (you cannot even see the lines for many of the points).

The second plot reports the Cook's D value as a function of an observation's Leverage. This essentially allows me to link high leverage points and those that are potentially overly influential. Here, we see that the high level point (observation 32, the United States) also is very influential.

### Distinction between Leverage and Influential Points

* Leverage quantifies the *potential* for a point to extert strong influence on the regression analysis, it is not necessarily wrong or an error.
* Leverage depends only on the predictor variables.
* Whether a point is influential or not depends on the observed value of the response (and potentially the leverage value).
* Points in the extremes (in the scatter plot, points to the far left or far right of the plot) of the predictor variables will always be more influential than those in the middle. But this does not necessarily mean there is an error.

### Final Thoughts on Leverage and Influential Points

First, it is important to note that the ideas of leverage and influential points exist in multiple regression but we did not explore that here today. It is not as easily visualized as was done here, but can be explored (we did so conceptually on on previous class!).

Second, what to do when you have a potential issues with a overly influential point.

* First, check for errors:
    + If the error is just a data entry or collection error and it can be corrected, do so!
    + If the data point is not representative of the intended population of study, delete it.
    + If the data point is a procedural error and invalidates the measurement, delete it.
* Perhaps you have the wrong model:
    + Did you leave out important predictor variables?
    + Should you consider adding interaction terms?
    + Is there any nonlinearity to be modeled? 
    + You could consider Weighted Least Squares (not difficult, but outsite the scope of this class).
* Decide on whether or not to delete observations:
    + In general it is recommend you not delete data unless highly justified
    + Do not delete data just because it does not fit your preconceived regression model. You are biasing your findings.
    + If you delete any data after collecting it, justification is absolutely necessary in any reports
    + If you are unsure on what to do with a point, consider analyzing the data twice -- once with and once without that particular datapoint (as we did above) -- report both analyses.
    + As an alternative to the above, you can use the dummy variable approach we did in the previous lecture. **This will form the basis of your in-class assignment.**
