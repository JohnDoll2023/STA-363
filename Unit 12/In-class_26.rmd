---
title: "Class 26: Inference in Logistic Regression"
author: "Hughes/Fisher"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```


**Revisit the spider cannibalism data!** Let's first get the data entered and the logistic regression model fit re-run from the end of the last session:

```{r}
spider <- read.csv("spiderCannibalism.csv")
spider.fit <- glm(Cannibalism ~ SizeDiff, data=spider, family=binomial(link=logit))
summary(spider.fit)
```


## Testing model terms/components

----

### Tests of individual predictors

**Wald z-tests.** Note in the summary output above, a $Z$-test for the significance of each of the predictor variables is provided (this example has only one predictor: `SizeDiff`). Here we see that the size difference between spiders appears to have a significant influence on the probability of cannibalism occuring in these spiders.

**Likelihood Ratio Chi-Square tests (preferred).** The Wald test is generally known to be statistically conservative (less Type I errors than is acceptable).  A generally preferred, and much more flexible, test is the LRT Chi-Square test.  The LRT test can be used to test the significance of individual predictors (even categorical predictors with multiple levels), sets of predictors (like the reduced *F*-test from earlier this semester), or "whole-model" tests.

To get LRT tests of individual predictors in your logistic regression model, use the `drop1()` function:

```{r}
drop1(spider.fit, test="Chisq")
```

There's only one test here because the model only has one predictor.  In a multiple predictor logistic regression, you would see a test for each individual predictor.  Here, we see that size difference is a significant predictor of cannibalism ($\chi^2$ = 18.942, $df$ = 1, $p$-value = 0.00001348).

----

### Whole-model test

We can test the overall significance of the complete model with a likelihood ratio test.  This is the corollary for logistic regression to the "whole-model *F-*-test" that we saw in regular linear regression.  We use the `anova()` function with the `LRT` test option:

```{r}
anova(spider.fit, test="LRT")
```

Note this is the same result that we got earlier.  Because this model only contains one predictor, the "whole-model" test is the same as the test for the single predictor.  The $\chi^2$-statistic here is labeled `Deviance`.

----

### Reduced-model tests

Suppose you had a model with four predictors `x1`, `x2`, `x3`, and `x4`.  If you wanted to test the single hypothesis at least one of `x3` or `x4` is significant, we can do it by performing an ANOVA-type test for comparing models. Recall the strategy: you would fit a model containing all four predictors (the *full* model) and a *reduced* model obtained by removing the subset of variables under investigation.  Comparing these models assesses the impact of `x3` and `x4` ... if the test result is significant, then at least one of `x3` or `x4` is important. In a logistic regression setting, the code for this would be

```{r, eval=FALSE}
full.model <- glm(response ~ x1 + x2 + x3 + x4, data=whatever, family=binomial(link=logit))
reduced.model <- glm(response ~ x1 + x2, data=whatever, family=binomial(link=logit))
anova(reduced.model, full.model, test="LRT")
```

This can be illustrated in the spider example by comparing a model with `SizeDiff` (the *full* model) to a model with no predictors at all (i.e. an intercept-only model ... this would be the *reduced* model in the illustration):

```{r}
full <- glm(Cannibalism ~ SizeDiff, data=spider, family=binomial(link=logit))
reduced <- glm(Cannibalism ~ 1, data=spider, family=binomial(link=logit))
anova(reduced, full, test="LRT")
```

This test tells us that the size difference does influence the probability of cannibalism occuring. You will also note this is the same as the above result except here it is done by comparing two different models. 

----

## Confidence intervals

We can also find confidence intervals for our estimated $\beta$-parameter values:

```{r}
confint(spider.fit)
```

It is **very important** to consider the interpretation of confidence intervals in logistic regression. These are confidence intervals for the coeffecients in the model **in terms of the log odds**.  If we want to get rid of the log part, we need to take exponents.

```{r}
exp(confint(spider.fit))
```

So a one unit change in size difference will increase the odds of cannibalism by between 4.22 and 245.18 (a huge range). But is important to note this change is all relative to the intercept. A one-unit increase in size may increase the odds by as much as 245 times, but the **original** odds were 0.006 to 0.174. So even in the most extreme case (in terms of 95\% confidence intervals), a one-unit increase in size difference will increase the odds from 0.1743 to 42.7 ($0.174\times 245$). 

**How does that affect the probability?** Good luck! We can complete the inverse logit transformation using the formula from last session:

```{r}
exp(confint(spider.fit))/(1+exp(confint(spider.fit)))
```

We see a range of probabilities, but interpretation is quite difficult here. The intercept has interpretation since this corresponds *specifically* to no size difference, where the probability of cannibalism is between 0.006 and 0.148. However, the size difference coefficient is pretty much meaningless here since the original model has been transformed so much.


----

### Fitted plot

Technically our response is the log odds. If we wanted to make a "fitted" plot, we need to transform the output from the `predict()` function.

```{r}
predict(spider.fit)
```

The output here are **model-predicted log-odds of cannibalism** for each of the individual data points (see both negative and positive values). We can get predicted probabilites instead with a minor change to the code:

```{r}
predict(spider.fit, type="response")
```

Now all values are between 0 and 1! 

Technically, a logistic function is a smooth S-curve. If we plotted the above values it would look fairly jagged, so to get a smooth curve we create a new dataset with many `SizeDiff` values (we did this before with log/exp of linear models) and then just run the `predict()` function on these values. By default, `predict()` outputs logs of odds, so we specify `type="response"`. 

```{r}
spider.fit.plot.data <- data.frame(SizeDiff=seq(-1.2, 2.2, 0.01))
spider.fit.plot.data <- spider.fit.plot.data %>% 
  mutate(Predicted.Prob = predict(spider.fit, newdata=spider.fit.plot.data, type="response"))

head(spider.fit.plot.data)

ggplot() + 
  geom_jitter(data=spider, aes(x=SizeDiff, y=Cannibalism), height=0, width=0.1, alpha=0.4) +
  geom_line(data=spider.fit.plot.data, aes(x=SizeDiff, y=Predicted.Prob)) + 
    labs(x="Size Difference (mm) (Female - Male)",
       y="Probability of Cannibalism", 
       title="Postmating Cannibalism in Wolf Spiders") + 
  theme_classic()
```

```{r, eval=FALSE, echo=FALSE}

ggplot(data=spider, aes(x=SizeDiff, y=Cannibalism)) + 
  geom_jitter(height=0, width=0.1, alpha=0.4) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) + 
    labs(x="Size Difference (mm) (Female - Male)",
       y="Probability of Cannibalism", 
       title="Postmating Cannibalism in Wolf Spiders") + 
  theme_classic()

```


**Can we perform multiple logistic regression?** Yes!!! You will see that in today's activity.  Before you do that, let's look at another example where the form of the data is presented differently.  And just to keep things consistent, we stick with the topic of cannibalism!


----

## Logistic regression from summarized data

With 0/1 binary response data, the response variable's values can be presented to R in one of two ways: by either a vector of 0's and 1's, or a matrix with two columns.  This latter way is sometimes the way data appears in studies where all the predictors are categorical. Consider the following depresing example (!):

### Example: The Donner Party

(Adapted from https://study.com/academy/lesson/the-donner-party-facts-summary.html):

By 1846, more and more people were picking up and moving west to states like Oregon, Utah and California. George Donner was the organizer of the Donner Party expedition. His advertisement in the Springfield, Illinois Gazette invited anyone who wanted to travel to California for free land to join him. Many families eager for the promise of land answered Donner's call to pursue the American dream.

On April 16, 1846, a group of 87 men, women and children left Illinois and began the long journey to California with the hope of arriving before the winter snows began in the Sierra Nevada Mountains.

<div style="float:right;position: relative; ">
```{r, out.width = "200px", echo=FALSE}
knitr::include_graphics("http://www.users.miamioh.edu/hughesmr/hastingscutoff.gif")
```
</div>

The Donner Party made a fateful error when they decided to try a untested shortcut known as the Hastings Cutoff.  Normally, travelers went north around the Great Salt Lake into Idaho before turning south again back into Nevada. However, a new route promising a straighter and faster passage south of the Great Salt Lake was suggested by land promoter Lansford Hastings in his book, *The Emigrants' Guide to Oregon and California*. As it turns out, Hastings had never actually traveled this route and had no idea if wagons could handle the trail.

In actuality, the cutoff was not accessible for wagons and cattle, and was many miles longer. The "shortcut" put the Donner Party weeks behind schedule, and when they reached the eastern side of the Sierra Nevada Mountains in late October, an early blizzard hit and snowed them in on Truckee Lake. It would turn out to be one of the worst winters in the history of the Sierra Nevadas where the snowpack can pile up to 30 feet deep. For five months, the Donner Party was stranded with no food, and no chance for escape or rescue.  They butchered the cattle they had, but many of the cows had wandered off or died in the snow. They soon turned to boiling leather shoestrings and hides for food, and as members of the party succumbed to the cold, some of the survivors tragically resorted to cannibalism of their friends and family.

The present data set contains actual information about survival counts by gender for the 87 members of the Donner party.

```{r}
donner <- read.csv("donnerparty.csv")
print(donner)
```

As you can see, the data are not presented as 87 rows (one per subject) as we are used to seeing.  Instead, the response information is summarized in two columns, where one column contains a summary total count of "successes" for the given row, and the other column contains a summary total count of "failures" for the given row.  In this problem, there are only two genders, so we have a two rows of success/failure counts, one for each gender.

Fortunately, handling this format for logistic regression data is easy in R.  All we need to do is make a slight change in the `glm` statement whereby we **bind** the two columns together in a matrix using the `cbind` function.  Here is the code for doing the logistic regression using this format, modeling the probability of survival for Donner Party members based on their gender:

```{r}
donner.fit <- glm(cbind(survived, died) ~ gender, data=donner, family=binomial(link=logit))
```

Note that the general format is `cbind(successcolumn, failurecolumn)`.

Some quick analysis:

**LRT test to see if $P(survival)$ is related to gender:** *(done as a "whole-model" test)*

```{r}
anova(donner.fit, test="LRT")
```

The probability of survival in the Donner Party is significantly different between the genders ($\chi^2$ = 5.076, $p$-value = 0.02426).

**Parameter estimate interpretation:**

```{r}
summary(donner.fit)
```

* $\hat{\beta_1}$ = `r coef(donner.fit)[2]`: The `gender` predictor here is expressed as an indicator variable of being **male**.  Since the parameter estimate is negative, it tells us that the log odds of survival for males is *smaller* than for females.  Re-expressing as odds:

```{r}
exp(coef(donner.fit)[2])
```

The odds of a male surviving is 0.363 times that of a female surviving. From an inferential perspective,

```{r}
exp(confint(donner.fit))
```

We can be 95% confident that a male is 0.142 to 0.878 times as likely to survive as compared to a female. (Note that you can "flip" this and re-express from the female size by taking reciprocals of the odds: We can be 95% confident that a female is 1.138 to 7.042 times as likely to survive as compared to a male.)

*QUESTION*: Interpret the confidence interval for the intercept $\beta_0$ above.


**Predicted probabilites of survival:**

```{r}
predict(donner.fit, type="response")
```

The model predicts that males have a 44.23% probability of survival; for females, the survival probability is estimated at 68.57%; which corresponds to the exact probabilities within the two groups.

