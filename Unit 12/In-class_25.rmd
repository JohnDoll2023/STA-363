---
title: "Class 25: Introduction to Logistic Regression"
author: "Hughes/Fisher"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning= FALSE)
library(tidyverse)
```

### Review: Last session

* What is odds? <br/><br/>
  expressing likelihood of events by prob of success/prob of failure which goes from 0 to infinity
* What is an odds ratio?<br/><br/>
  ratio of two different odds, usually under 2 different circumstances
* What is the benefit of considering log odds?<br/><br/>
  the log ratio goes from negative infinity to positive infinity and helps eliminate restraints for modeling

We hinted at logistic regression last session. This session, we formally introduce it through an example. But we will start today by introducing the concept of generalized linear models.  

----

## Generalized Linear Models

For most of this course, we have been focused on linear models (`lm`). One of the key assumptions we have been making with linear models is that the residuals are normally distributed. But not all models are based on normally distributed errors. 

**Generalized linear models** (`glm`) allow for other types of error distributions. GLMs consist of two additional components:

* A link function: "links" the linear model form to the response variable
* Family: the distribution of the response variable

Linear regression is a special case of GLM with

* Link: Identity, $f(x)=x$
* Family: Normal Disribution

Logistic regression is another type of GLM. In this case:

* Link: log odds, $f(p)=\log\left(\frac{p}{1-p}\right)$
    + This is also called the *logit* link function
* Family: Binomial Distribution

We will touch on a third type of GLM, *Poisson regression*, next week.

----

## Logistic Regression Example - Spiders!

You may begin by watching this short introductory video:

<iframe width="560" height="315" src="https://www.youtube.com/embed/fwUpjWKAsik?rel=0" frameborder="0" allowfullscreen></iframe>

### Data Description

The paper "Sexual Cannibalism and Mate Choice Decisions in Wolf Spiders: Influence of Male Size and Secondary Sexual Characteristics" (Animal Behaviour [2005]: 83-94) described a study in which researchers were interested in variables that might be related to a female wolf spider's decision to kill and consume her partner during courtship or mating. The accompanying data (approximate values read from a graph in the paper) are values of difference in body width (female - male) and whether cannibalism occurs, coded as 0 for no cannibalism and 1 for cannibalism for 52 pairs of courting wolf spiders.

Source: "Introduction to Statistics and Data Analysis" by Roxy Peck, Chris Olsen, Jay L. Devore.

### Some data visualization

We have a binary response (cannibalism or not) and a single predictor (continuous variable, difference in size), so this allows us to be creative with visual displays. Below are two examples:

```{r}
spider <- read.csv("spiderCannibalism.csv")
tail(spider)
```

Note the response variable, `Cannibalism` is recorded as a 1 or 0. Treating this as a numeric variable has some benefits for certain plots. For other plots, we may want to treat it categorically, so let's make a 'copy'. An example is below:

```{r}
spider <- spider %>% 
  mutate(Male.consumed=as.logical(Cannibalism))

ggplot(spider) + 
  geom_density(aes(SizeDiff, fill=Male.consumed), alpha=0.2)+
  labs(x="Size Difference (mm) (Female - Male)", 
       title="Postmating Cannibalism in Wolf Spiders") +
  theme_classic()
```

In the above plot, we segment the data into two groups (those where cannibalism occured and those without) and look at the distribution of the size difference variable.  This provides some insight into what is happening. Notice we see that the `Male.consumed` group generally has larger size differences than the group where the male was not eaten.  Keep in mind here that in this plot we are really looking at things backwards (essentially how does $Y$ influence $X$).

Another way to visualize this data is the following:


```{r}
ggplot(spider) + 
  geom_point(aes(x=SizeDiff, y=Cannibalism) ) + 
  theme_classic()
```


Here we use the numeric version of the response, `Cannibalism`, instead of the categorical version `Male.consumed` we used earlier. Thus, we get 1 and 0 values on the plot. We also note it is difficult to parse out the individual observations: due to rounding of the size differences, many points are stacked on top of one another. One graphical way to handle this is with *jittering* -- wherein we add a little bit of a random "shake" to the data:

```{r}
ggplot(spider) + 
  geom_jitter(aes(x=SizeDiff, y=Cannibalism), height=0, width=0.1, alpha=0.4) + 
  labs(x="Size Difference (mm) (Female - Male)",
       y="Cannibalism occured", 
       title="Postmating Cannibalism in Wolf Spiders") + 
  theme_classic()
```

Note the use of `geom_jitter` instead of `geom_point`. Although the `sizeDiff` variable is continuous, in the data it has been discretized (rounded to closest first decimal in increments of 2). By adding the jittering option (which basically adds some randomness to the observations), it spreads out the data (visually) so you can see some separation. We also use the `alpha` scaling option (where overlapping points will show up darker than stand-alone points) so we can see little clusters. We also cleaned up the labeling.

Visually it looks like larger size differences result in more cases of spider cannibalism. How can we model this?  The answer is Logistic Regression.

----

### Modeling

How do we model this?

First, let's recall some material from last session. In this example, our **response** variable is a **binary** outcome (True/False on whether postmating cannibalism occured). In this sort of experiment, the parameter of interest is $p$ defined as

$$p = P(\textrm{Postmating Cannibalism Occured})$$

Since $p$ is a probability (or proportion), we know $0 \leq p \leq 1$, and thus standard linear regression is not appropriate. But we saw in the last session that if we instead consider the *odds* of cannibalism,

$$odds = \frac{p}{1-p}$$

we are now working in the domain $(0, \infty)$. Further, if we consider the logarithm of the odds, that is $\log\left(\frac{p}{1-p}\right)$, we are working in the domain $(-\infty, \infty)$. Thus, linear regression is more valid when working with the log-odds, so we could consider the model

$$logit(p) = \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X$$

Last session, we worked through an example modeling prevelence of heart disease as a function of BMI. Here, we want to do the same thing! In terms of $p$, the above model can be written as

$$p = \frac{\exp\left(\beta_0 + \beta_1 X\right)}{1 + \exp\left(\beta_0 + \beta_1 X\right)}$$

The above is known as a *logistic equation*. This is why this is called *logistic regression*.


### Modeling in R

To fit a logistic regression in R, we use the `glm` function, which stands for **generalized linear model**. In our derivation above, we are *generalizing* linear regression to work with some different link function (specified with the `link` option) of the data ... in this example, a logistic regression. Below is how we fit our model:

```{r}
spider.fit <- glm(Cannibalism ~ SizeDiff, data=spider, family=binomial(link=logit))
```

That's it! The code is very similar to our `lm()` and `aov()` code from before. The key thing with `glm()` is we need to specify the distribution of the response (the family). We can also specify the link function (this is technically not necessary here because `link=logit` is the default behavior for the binomial family), in this example that is `family=binomial`.

We can then explore the output of the fitted model:

```{r}
summary(spider.fit)
```

We see the output is very similar to the `lm()` function we have used throughout the semester. Below we describe important facts about the output.

----

### Interpretation of parameters

* $\hat{\beta_0}$ = `r coef(spider.fit)[1]` 
    + Negative, so when size difference = 0, log-odds of cannibalism will be negative
    + This means the odds are less than 1
    + So the probability of cannibalism when there is no size difference is less than 0.5
    + Specifically, the odds is $\frac{p}{1-p} = \exp(\beta_0)$ = `r exp(coef(spider.fit)[1])` which is another way of saying the probability of cannibalism when there is no size difference is `r exp(coef(spider.fit)[1])/(1+exp(coef(spider.fit)[1]))`.  
    + So if the spiders are the same size, there is only about a 4\% chance the female will kill and eat the male spider.

* $\hat{\beta_1}$ = `r coef(spider.fit)[2]`
    + Positive, so larger size difference leads to more cannibalism
    + For every 1 unit increase in size difference, we would expect the log odds to increase by `r coef(spider.fit)[2]` units. 
    + This gets a little weird when you try to interpret in terms of the the probability, since we are modeling the log odds.
    + Think back to algebra: sums of logs are multiplication of exp terms, so all increases are relative to the *intercept* term. The odds $\frac{p}{1-p}$ will grow at a multiplicative rate of `r exp(coef(spider.fit)[2])` for each one-unit increase in the size difference. 

**Further discussion** -- So if the female is 1 unit larger than the size, we basically have the intercept and slope cancelling out ($-3.089 + 3.069(1) \approx  0$), and this results in a log odds near 0, which corresponds to the odds being near 1, and a probability of about 0.5 for cannibalism to occur.

**Other considerations**: Note an AIC value is reported in the `summary()` output. It works just as before! We also see $z$-tests and corresponding $p$-values. These are known as the Wald tests for parameter significance.  More on these next time.

The "deviance" values are essentially measures of variability. The `Null deviance` can be considered the variability in the original data, whereas the `Residual deviance` can be considered the variability in the residuals. Using these values, we can construct pseudo-$R^2$ values and model comparisons similar to an $F$-test (done next time).

Many of the methods we have learned throughout the semester are still valid: predictive modeling, variable selection, etc. Some of these topics will be explored later.

